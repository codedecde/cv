%
% LaTeX source of my resume
% =========================
%
% Heavily commented to to fit even LaTeX beginners (hopefully).
%
% See the `README.md` file for more info.
%
% This file is licensed under the CC-NC-ND Creative Commons license.
%

%\\TODO
%   - make more spacious, change margins and line spacing
%   - add PoR etc.
%   - center name etc.

% Start a document with the here given default font size and paper size.
\documentclass[8pt,a4paper]{article}

% Set the page margins.
\usepackage[a4paper,margin=0.5in]{geometry}

% Bibliography imports
\usepackage{bibentry}
\makeatletter\let\saved@bibitem\@bibitem\makeatother

% Color the hyperlinks
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\makeatletter\let\@bibitem\saved@bibitem\makeatother
% Setup the language.
\usepackage[english]{babel}
\hyphenation{Some-long-word}

% Makes resume-specific commands available.
\usepackage{resume}

% SNAIR packages
\usepackage{enumitem} % to move itemize items a little to the left
\setlist[itemize]{leftmargin=0mm}

\usepackage{multicol}


\title{CV for Industries}
\begin{document}  % begin the content of the document
\nobibliography{references}
\bibliographystyle{unsrt}
\sloppy  % this to relax whitespacing in favour of straight margins

\begin{center}
% title on top of the document
\maintitle{Barun Patra}{}{}

\nobreakvspace{0.3em}  % add some page break averse vertical spacing

% \noindent prevents paragraph's first lines from indenting
% \mbox is used to obfuscate the email address
% \sbull is a spaced bullet
% \href well..
% \\ breaks the line into a new paragraph
%\sbull
\noindent E-Mail : \href{mailto:barunpatra95@gmail.com}{ \nolinkurl{barunpatra95@gmail.com} } $\mid$  Github : \href{https://github.com/codedecde}{\nolinkurl{codedecde@github.com} } $\mid$ \noindent Mobile No. : (425) 465 7261 \\
\noindent Website: \href{https://www.microsoft.com/en-us/research/people/bapatra}{ \nolinkurl{https://www.microsoft.com/en-us/research/people/bapatra}} \\
\noindent Google Scholar: \href{https://scholar.google.com/citations?hl=en&user=Gwg25AkAAAAJ}{\nolinkurl{https://scholar.google.com/citations?hl=en&user=Gwg25AkAAAAJ}}
\end{center}
\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after
\roottitle{Research Interests}
\bodytext{
  My research interest focuses on pre-training large models for multi-modal and multilingual applications, with an emphasis on inference speed, novel abilities like large context understanding and alignment to human preference. My work revolves around all aspects of pre-training: scaling data and models, exploring novel model variants for improving inference speed, sparse scaling and enabling larger context processing, as well as post-training alignment research. I am also interested in the limitations of such large models, and improving their robustness and interpretability.
}
\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after
\roottitle{Education} 
\headedsection
  {\textbf{Carnegie Mellon University, Pittsburgh}}
  {\textsc{August 2017 - December 2018}} {%
  
    {
    \emph{Master's of Science in Machine Learning} \\
    Cumulative Grade Point Average: 4.17
    }
    
}
\headedsection
  {\textbf{Indian Institute of Technology, Delhi}}
  {\textsc{July 2013 - June 2017}} {%
  
    {
    \emph{B.Tech in CS\&E with a Specialization in Data Analytics and AI} \\
    Cumulative Grade Point Average: 9.494/10
    }
    
}
\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after
\roottitle{{Work Experience}}
\headedsection
  {\textbf{Turing Team: Microsoft Vancouver, Microsoft Redmond}}
  {\textsc{Feb 2022 - Present, Nov 2020 - Feb 2022}} {%
  
    {
    \href{https://turing.microsoft.com}{Link to Team Description}  \hfill \textit{Senior Applied Scientist, Applied Scientist II}  \\
    I work on building better representation and generative foundational multilingual and multimodal language models. The work is instrumental in pushing frontiers on academic benchmarks (\href{https://blogs.bing.com/search-quality-insights/october-2022/Microsoft-Turing-Universal-Language-Representation-model,-T-ULRv6,-tops-both-XTREME-and-GLUE-leaderb}{$1^{st}$ on both XTREME and GLUE benchmarks}), as well as cutting edge user experiences like \href{https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/}{Bing Chat}. My work spans all aspects of building these models: processing petabyte scale data, scaling models to billions of parameters, exploring novel ways to reduce latency, developing different pre-training strategies as well as post-training strategies to improve the alignment of these models. I also work closely with different university research labs further advance research on understanding the capabilities and shortcomings of these models (\href{https://www.microsoft.com/en-us/research/collaboration/microsoft-turing-academic-program/}{Microsoft Turing Academic Program}).
    
    }
}
\headedsection
  {\textbf{Scheduler Team, Microsoft Redmond}}
  {\textsc{May 2018-Aug 2018, Feb 2019 - Nov 2020}} {%

    {
      \href{https://www.microsoft.com/en-us/research/project/scheduler/}{Link to Team Description} \hfill \textit{Applied Scientist Intern, Applied Scientist} \\
      I developed NLP components for email scheduling assistant, including intent classification, entity extraction, time understanding, negation handling and understanding spans of interest in emails. This consequently improved automation levels from below 20\% to over 90\% and contributed to 2 publications and 4 patents. I also organized NLP workshops and reading groups.
    }
}
\headedsection
  {\textbf{Masters in ML, CLU}}
  {\textsc{Aug 2017 - Dec 2018}} {%
  {
    \\ During my Masters, I had the opportunity to work with Professor Matthew Gormley, Professor Barnab\`as P\'oczos and Professor Graham Neubig. Noteworthy projects include Knowledge Based completion using an RL agent with Monte Carlo Tree Search for generating supervised policies for the agent, where the resulting method yielded state-of-the-art results on 3 popular KB datasets, model compression for a DQN network (down to 3\% of the original parameters while retaining performance) and quantifying degree of mis-alignement between embedding spaces of different languages and providing a semi-supervised framework for aligning embedding spaces of different languages.
  }
}
\headedsection
  {\textbf{DAIR, IIT Delhi}}
  {\textsc{July 2016 - July 2017}} {%
  {
    \\I was a part of the Data Analytics and Intelligence Research Group lab and was advised by Professor Mausam and Professor Parag Single. During my time there, I worked on incorporating global logical constraints to CRF inference for tagging entities of interest and using it for semi-supervised learning for answering multi-sentence tourism related questions. I also worked on automated dialogue system evaluation, leveraging the thread structure of Reddit to generate context, response and alternate response triples, with the final model achieving a zero-shot Pearson correlation of 0.45 with human scores on the Ubutnu dataset.
  }
}
\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after
\roottitle{{Teaching Experience}}

\headedsection
  {\textbf{Advanced Introduction to ML (CMU)}}
  {\textsc{Fall 2018}}{%
    {
      \\\textit{Teaching Assistant under the guidance of Professor Nina Balcan}\\Worked on developing questions for assigments and exams, graded exams, provided mentorship and recitations for 70+ primarily PhD students
    }
  }

\headedsection
  {\textbf{Artificial Intelligence (IIT Delhi)}}
  {\textsc{Spring 2017}}{%
    {
      \\\textit{Teaching Assistant under the guidance of Professor Mausam}\\Graded exams and quizzes and developed assigments for 125+ students in this grad/undergrad bridge course
    }
  }
\pagebreak
\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after
\roottitle{{Academic Services}}
I have served as a reviwer for the following conferences
\begin{multicols}{3}
\begin{itemize}[label={}]
% \setlength\itemsep{0.0 em} % for line spacing in the list
\item ACL Rolling Reviews (2021, 2022, 2023)
\item EMNLP (2022$^{\dagger}$, 2023)
\item TPAMI (2023)
\item NeurIPS (2022, 2023)
\item ACL(2021, 2022)
\item ICLR (2022)
\item GHC (2020)
\item AAAI (2018)
\end{itemize}
\end{multicols}
\textit{$\dagger$ indicates outstanding reviewer}

\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after

\roottitle{{Patents}}
\bodytext{
    \begin{itemize}[label={}]
    \setlength\itemsep{0.0 em} % for line spacing in the list
    \item Patra Barun, Fufa Chala, Bhattacharya Pamela, Lee Charles. Identifying contextual and task relevant time entities with constraints	(MS\# 408676-US-NP), filed on 09/18/2020
    \item Suryanarayanan Vishwas, Patra Barun, Bhattacharya Pamela, Fufa Chala, Lee Charles. Artificial Intelligence For Identifying Relevant Content Related To Specific Tasks (MS\# 407907-US-NP), filed on 12/06/2019
    \item Patra Barun, Bhattacharya Pamela, Lee Charles. Resolving Temporal Ambiguities in Natural Language Inputs Leveraging Syntax Tree Permutations (MS\# 407193-US-NP), filed on 9/16/2019
    \item Patra Barun, Bhattacharya Pamela, Lee Charles. Sentence Attention Modeling For Event Scheduling via Artificial Intelligence and Digital Assistants (MS\# 405393-US-NP), filed on 11/30/2018
    \end{itemize}
}

\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after

\roottitle{{Publications and Preprints}}
 \bodytext{
 \begin{itemize}[label={}]
    %[label=\raisebox{0.25ex}{\tiny$\bullet$}]
    \setlength\itemsep{0.0 em} % for line spacing in the list
    \item \href{https://aclanthology.org/2023.acl-tutorials.3/}{\textbf{Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world}} Sunayana Sitaram, Monojit Choudhury, Barun Patra, Vishrav Chaudhary, Kabir Ahuja, Kalika Bali. (\textbf{Tutorial @ ACL 2023})
    \item \href{https://arxiv.org/abs/2302.14045}{\textbf{Language Is Not All You Need: Aligning Perception with Language Models}} Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei. (\textbf{arXiv:2302.14045})
    \item \href{https://arxiv.org/abs/2210.14867}{\textbf{Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning }} Barun Patra$^{*}$, Saksham Singhal$^{*}$, Shaohan Huang$^{*}$, Zewen Chi, Li Dong, Furu Wei, Vishrav Chaudhary and Xia Song. (\textbf{ACL 2023})
    \item \href{https://arxiv.org/abs/2212.10554}{\textbf{A Length-Extrapolatable Transformer}} Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei. (\textbf{ACL 2023})
    \item \href{https://arxiv.org/abs/2210.06423}{\textbf{Foundational Transformers}} Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song and Furu Wei. (\textbf{ICML 2023})
    \item \href{https://arxiv.org/abs/2210.07228}{\textbf{Language Model Decoding as Likelihood-Utility Alignment}} Martin Josifoski, Maxime Peyrard, Frano Rajic, Jiheng Wei, Debjit Paul, Valentin Hartmann, Barun Patra, Vishrav Chaudhary, Emre Kiciman, Boi Faltings, Robert West. (\textbf{EACL 2023})
    \item \href{https://arxiv.org/abs/2211.13184}{\textbf{TorchScale: Transformers at Scale}} Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, Furu Wei. (\textbf{arXiv:2211.13184})
    \item \href{https://aclanthology.org/2022.sumeval-1.1.pdf}{\textbf{The SUMEval 2022 Shared Task on Performance Prediction of Multilingual Pre-trained Language Models}} Kabir Ahuja, Antonios Anastasopoulos, Barun Patra, Graham Neubig, Monojit Choudhury, Sandipan Dandapat, Sunayana Sitaram, Vishrav Chaudhary. (\textbf{AACL 2022})
    \item \href{https://arxiv.org/pdf/2204.09179.pdf}{\textbf{On the Representation Collapse of Sparse Mixture of Experts}} Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, Furu Wei. (\textbf{NeurIPS 2022})
    \item \href{https://arxiv.org/pdf/2110.08413.pdf}{\textbf{Invariant Language Modeling }} Maxime Peyrard, Sarvjeet Singh Ghotra, Martin Josifoski, Vidhan Agarwal, Barun Patra, Dean Carignan, Emre Kiciman, Robert West, (\textbf{EMNLP 2022})
    \item \href{https://arxiv.org/abs/2204.01016}{\textbf{On Efficiently Acquiring Annotations for Multilingual Models}} Barun Patra$^{*}$, Joel Ruben Anthony Moniz$^{*}$, Matthew R Gormley. (\textbf{ACL 2022})
    \item \href{https://arxiv.org/pdf/2003.04988.pdf}{\textbf{ScopeIt: Scoping Task Relevant Sentences in Documents}}: Barun Patra$^{*}$, Vishwas Suryanarayanan$^{*}$, Pamela Bhattacharya, Chala Fufa, Charles Lee. Proceedings of the 28th International Conference on Computational Linguistics \textbf{(COLING 2020, Industry Track)}
    \item \href{https://www.aclweb.org/anthology/2020.emnlp-main.678.pdf}{\textbf{To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints}}: Barun Patra, Chala Fufa, Pamela Bhattacharya and Charles Lee. \textbf{(EMNLP 2020)}
    \item \href{https://www.cambridge.org/core/journals/natural-language-engineering/article/constrained-bert-bilstm-crf-for-understanding-multisentence-entityseeking-questions/891F2A9DAD6642A17D8B67523EC45316}{\textbf{Constrained BERT BiLSTM CRF for Understanding Multi-Sentence Entity Seeking Questions}}: Danish Contractor, Barun Patra, Mausam and Parag Singla. \textbf{(JNLE 2020)}
    \item \href{https://www.aclweb.org/anthology/D19-1652.pdf}{\textbf{Weakly Supervised Attention Networks for Entity Recognition}}: Barun Patra$^{*}$ and Joel Ruben Anthony Moniz$^{*}$. (\textbf{EMNLP-IJCNLP 2019})
    \item \href{https://www.aclweb.org/anthology/P19-1018.pdf}{\textbf{Bilingual Lexicon Induction with Semi-Supervision in Non-Isometric Embedding Spaces}}: Barun Patra$^{*}$, Joel Ruben Anthony Moniz$^{*}$, Sarthak Garg$^{*}$, Matthew R Gormley and Graham Neubig. (\textbf{ACL 2019})
    \item \href{https://pdfs.semanticscholar.org/90aa/b652eed53d7a5b8284c173b1aac698a43e4c.pdf}{\textbf{Understanding complex multi-sentence entity seeking questions}}: Danish Contractor, Barun Patra, Mausam and Parag Singla. (\textbf{AAAI Reasoning for Complex Question Answering Workshop, 2018}).
    \item \href{https://arxiv.org/pdf/1904.09489.pdf}{\textbf{Compression and Localization in Reinforcement Learning for ATARI Games}}: Joel Ruben Anthony Moniz$^{*}$, Sarthak Garg$^{*}$ and Barun Patra$^{*}$. (\textbf{NeurIPS Deep Reinforcement Learning Workshop, 2018})
    \item \href{https://arxiv.org/abs/1705.04009}{\textbf{A Survey of Community Question Answering}} Barun Patra. (\textbf{arXiv:1705.04009})
  \end{itemize}
  $^{*}$ denotes equal contribution
}

\spacedhrule{0.6em}{-0.4em}  % a horizontal line with some vertical spacing before and after

\roottitle{{Scholastic Achievements}}

\bodytext{%
    \begin{itemize}[label={}]
        \setlength\itemsep{0.0 em} % for line spacing in the list
        \item {\bf Department Rank 3} : Top 3\% among students of Computer Science and Engieering Department, Batch of 2017
        \item {\bf Suresh Chandra Memorial Trust Bachelor's Thesis Award} (2017) : Best B.Tech Project
        \item \textbf{Narotam Sekhsaria Postgraduate Scholarship (NSF Foundation)} (2017) : Selected as one of the top 18 students in India pursuing Pure and Applied Sciences, Social Sciences, Law, Architecture and Management.
        \item \textbf{KC Mahindra Scholarship} (2017) : Selected as one of the top 50 students in India for pursuing post-graduate studies abroad in varied fields
        \item \textbf{Merit Scholarship} For being in the top 7\% for 5 semesters.
    \end{itemize}
    
}

\end{document}

